{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Quick Task.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RRbdQK_l4iB7",
        "outputId": "41722fa4-c641-4d16-e2f1-32be48515a98"
      },
      "source": [
        "import os\n",
        "import io\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import string\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "train_data_path = os.path.join(os.getcwd(), \"gdrive\", \"My Drive\", \"QT\", \"train.txt\")\n",
        "test_data_path = os.path.join(os.getcwd(), \"gdrive\", \"My Drive\", \"QT\", \"test.txt\")\n",
        "file = os.path.join(os.getcwd(), \"gdrive\", \"My Drive\", \"QT\", \"Tencent_AILab_ChineseEmbedding.txt\")\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ywiYOvAcT0Zm",
        "outputId": "b3fd35a3-99dc-4f75-c44c-3838fe1874f3"
      },
      "source": [
        "import gensim.downloader as apiâ€©\n",
        "glove = api.load('glove-twitter-200')"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[=================================================-] 99.3% 752.9/758.5MB downloaded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U72rkcHrZ-lq"
      },
      "source": [
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "wv_chinese = KeyedVectors.load_word2vec_format(file, binary=False)"
      ],
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYSbdNy88M57"
      },
      "source": [
        "with io.open(train_data_path, encoding='utf8') as training_f:\n",
        "        training = training_f.read().split(\"\\n\")[0:-1]\n",
        "with io.open(test_data_path, encoding='utf8') as testing_f:\n",
        "        testing = testing_f.read().split(\"\\n\")[0:-1]"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hHGWwAHxc501",
        "outputId": "b84b32ea-71ec-42bb-9805-689cd442fd05"
      },
      "source": [
        "lst = str.split(training[18])\n",
        "print(len(wv_chinese[lst]),len(lst))"
      ],
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "49 49\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "317zDHGp-d7F"
      },
      "source": [
        "# 1.Preprocess Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCEvAbl6U26Y"
      },
      "source": [
        "UNK = \"<UNK>\"\n",
        "def make_vocab(sentences):\n",
        "    \"\"\"make_vocab creates a set of vocab words that the model knows\n",
        "\n",
        "    :param data: The list of documents that is used to make the vocabulary\n",
        "    :type data: List[str]\n",
        "    :returns: A set of strings corresponding to the vocabulary\n",
        "    :rtype: Set[str]\n",
        "    \"\"\"\n",
        "    vocab = set()\n",
        "    for sentence in sentences:\n",
        "      for word in sentence:\n",
        "        vocab.add(word)\n",
        "    return vocab\n",
        "def make_indices(vocab):\n",
        "\t\"\"\"make_indices creates a 1-1 mapping of word and indices for a vocab.\n",
        "\n",
        "\t:param vocab: The strings corresponding to the vocabulary in train data.\n",
        "\t:type vocab: Set[str]\n",
        "\t:returns: A tuple containing the vocab, word2index, and index2word.\n",
        "\t\tvocab is a set of strings in the vocabulary including <UNK>.\n",
        "\t\tword2index is a dictionary mapping tokens to its index (0, ..., V-1)\n",
        "\t\tindex2word is a dictionary inverting the mapping of word2index\n",
        "\t:rtype: Tuple[\n",
        "\t\tSet[str],\n",
        "\t\tDict[str, int],\n",
        "\t\tDict[int, str],\n",
        "\t]\n",
        "\t\"\"\"\n",
        "\tvocab_list = sorted(vocab)\n",
        "\tvocab_list.append(UNK)\n",
        "\tword2index = {}\n",
        "\tindex2word = {}\n",
        "\tfor index, word in enumerate(vocab_list):\n",
        "\t\tword2index[word] = index \n",
        "\t\tindex2word[index] = word \n",
        "\tvocab.add(UNK)\n",
        "\treturn vocab, word2index, index2word \n",
        "def remove_punc(text):\n",
        "    exclude = set(string.punctuation)\n",
        "    return ''.join(ch for ch in text if ch not in exclude)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YrndM2m5-f5Q"
      },
      "source": [
        "def preprocess(training, testing):\n",
        "  train_size, test_size = round(len(training)/6), round(len(testing)/6)\n",
        "  train = []\n",
        "  test = []\n",
        "  max_src, max_ref, max_can = 0, 0, 0\n",
        "  for i in range(train_size):\n",
        "    idx = 6*i\n",
        "    max_src = max(max_src, len(training[idx]))\n",
        "    max_ref = max(max_ref, len(remove_punc(training[idx+1])))\n",
        "    max_ref = max(max_ref, len(remove_punc(training[idx+1])))\n",
        "    comb = [str.split(training[idx]),word_tokenize(remove_punc(training[idx+1])),word_tokenize(remove_punc(training[idx+2])),training[idx+3],training[idx+4]]\n",
        "    train.append(comb)\n",
        "  for i in range(test_size):\n",
        "    idx = 6*i\n",
        "    comb = [str.split(testing[idx]),word_tokenize(remove_punc(training[idx+1])),word_tokenize(remove_punc(training[idx+2])),testing[idx+3],testing[idx+4]]\n",
        "    test.append(comb)\n",
        "  return train, test"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2L6BUQlfTFej"
      },
      "source": [
        "train, test  = preprocess(training,testing)\n",
        "train_source = [i[0] for i in train]\n",
        "train_refer = [i[1] for i in train]\n",
        "train_candi = [i[2] for i in train]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRHCxZl8Zq0k"
      },
      "source": [
        "#Create embedding matrix for both Chinese Characters and English Characters\n",
        "e_vocab = make_vocab(train_refer + train_candi)\n",
        "e_vocab, e_word2index, e_index2word = make_indices(e_vocab)\n",
        "c_vocab = make_vocab(train_source)\n",
        "c_vocab, c_word2index, c_index2word = make_indices(c_vocab)\n",
        "e_matrix, c_matrix = [], []\n",
        "e_len , c_len = len(e_vocab), len(c_vocab)\n",
        "e_matrix = np.zeros((e_len, 200))\n",
        "c_matrix = np.zeros((c_len,200))\n",
        "for i, word in enumerate(e_vocab):\n",
        "  if word in glove:\n",
        "    e_matrix[i] = glove[word]\n",
        "  else:\n",
        "    e_matrix[i] = np.random.normal(scale=0.6, size=(200, ))\n",
        "for i, word in enumerate(c_vocab):\n",
        "  if word in wv_chinese:\n",
        "    c_matrix[i] = wv_chinese[word]\n",
        "  else:\n",
        "    c_matrix[i] = np.random.normal(scale=0.6, size=(200, ))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EeUpfkrrXtak"
      },
      "source": [
        "path = F\"/content/gdrive/My Drive/QT/\"\n",
        "e_vocab = make_vocab(train_refer + train_candi)\n",
        "e_vocab, e_word2index, e_index2word = make_indices(e_vocab)\n",
        "c_vocab = make_vocab(train_source)\n",
        "c_vocab, c_word2index, c_index2word = make_indices(c_vocab)\n",
        "e_matrix = np.load(path+\"e_matrix.npy\")\n",
        "c_matrix = np.load(path+\"c_matrix.npy\")"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtX2jd6j8F-r"
      },
      "source": [
        "def transform(data):\n",
        "  x = []\n",
        "  y = []\n",
        "  for src, ref, can, score, label, in data:\n",
        "    vecs_src=[]\n",
        "    for word in src:\n",
        "      if word in c_vocab:\n",
        "        vecs_src.append(c_matrix[c_word2index[word]])\n",
        "    vecs_src = np.array(vecs_src)\n",
        "    vec_src = vecs_src.mean(axis=0)\n",
        "    vecs_ref = []\n",
        "    for word in ref:\n",
        "      if word in e_vocab:\n",
        "        vecs_ref.append(e_matrix[e_word2index[word]])\n",
        "    vecs_ref = np.array(vecs_ref)\n",
        "    vec_ref = vecs_ref.mean(axis=0)\n",
        "    vecs_can = []\n",
        "    for word in can:\n",
        "      if word in e_vocab:\n",
        "        vecs_can.append(e_matrix[e_word2index[word]])\n",
        "    vecs_can = np.array(vecs_can)\n",
        "    vec_can = vecs_can.mean(axis=0)\n",
        "    comb = np.concatenate((vec_src,vec_ref), axis=None)\n",
        "    comb = np.concatenate((comb,vec_can), axis=None)\n",
        "    comb = np.concatenate((comb,score), axis=None)\n",
        "    x.append(comb)\n",
        "    y.append(1 if label==\"H\" else 0)\n",
        "  return x,y\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2md4uNh2GXKe"
      },
      "source": [
        "xTrain, yTrain =transform(train)\n",
        "xTest, yTest = transform(test)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLw9q1SdJVcB"
      },
      "source": [
        "# 2.Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VUyJe9JwHqTd"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "parameters = { \n",
        "    'C': [0.01,0.1,1.0, 10,100,1000,10000],\n",
        "    'gamma': [0.01,0.1,1,10,100, 'auto', 'scale']\n",
        "}\n",
        "model = GridSearchCV(SVC(kernel='rbf'), parameters, cv=5, n_jobs=-1).fit(xTrain, yTrain)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GDzapTlxKB58",
        "outputId": "ff21483c-378e-4bbd-ba30-225203e28085"
      },
      "source": [
        "yPred = model.predict(xTest)\n",
        "print(np.count_nonzero(yPred==yTest)/len(yTest))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.6839080459770115\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ak4TkYFtdSWN"
      },
      "source": [
        "# 3.Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MUMr-ZLgLI_h",
        "outputId": "e5e06ea2-12d5-400c-cbe1-f64431d36a7d"
      },
      "source": [
        "from sklearn.metrics import f1_score\n",
        "print(f1_score(yPred,yTest))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7058823529411765\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}